{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bc8426-b37e-446d-8763-843d2296630b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4837f5ef-fd2e-46ff-8837-23510e3828a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Função para carregar o conjunto de dados MNIST\n",
    "def load_mnist(image_file, label_file):\n",
    "    with open(label_file, 'rb') as lbpath:\n",
    "        lbpath.read(8)\n",
    "        labels = np.frombuffer(lbpath.read(), dtype=np.uint8)\n",
    "\n",
    "    with open(image_file, 'rb') as imgpath:\n",
    "        imgpath.read(16)\n",
    "        images = np.frombuffer(imgpath.read(), dtype=np.uint8).reshape(len(labels), 784)\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fdf15f7a-73d1-41e1-a15a-28c834f52ed5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one hot\n",
    "def one_hot_convert(vec):\n",
    "    matrix = []\n",
    "    for idx in vec:\n",
    "      m = np.zeros((10, 1))\n",
    "      m[idx] = 1\n",
    "      matrix.append(m)\n",
    "    return np.array(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f620b254-df05-44bc-a64f-131207cfd41c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ORIGINAL\n",
      "train_images (60000, 784)\n",
      "train_labels (60000,)\n",
      "test_images (10000, 784)\n",
      "test_labels (10000,)\n"
     ]
    }
   ],
   "source": [
    "# LEITURA DOS DADOS\n",
    "train_images, train_labels = load_mnist('dataset/train-images.idx3-ubyte', 'dataset/train-labels.idx1-ubyte')\n",
    "test_images, test_labels = load_mnist('dataset/t10k-images.idx3-ubyte', 'dataset/t10k-labels.idx1-ubyte')\n",
    "\n",
    "print('ORIGINAL')\n",
    "print('train_images',train_images.shape)\n",
    "print('train_labels',train_labels.shape)\n",
    "print('test_images',test_images.shape)\n",
    "print('test_labels',test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a7cf252d-1264-4896-8e28-a365a9cb10ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 785) (60000, 10)\n",
      "(10000, 785) (10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Transformar as imagens em vetores e normalizá-las\n",
    "X_train = train_images.reshape(train_images.shape[0], -1)\n",
    "X_test = test_images.reshape(test_images.shape[0], -1)\n",
    "\n",
    "# Adicionar uma coluna de 1s para considerar o termo de bias (intercept) no modelo linear\n",
    "X_train = np.column_stack((X_train, np.ones(X_train.shape[0])))\n",
    "X_test = np.column_stack((X_test, np.ones(X_test.shape[0])))\n",
    "\n",
    "y_train = one_hot_convert(train_labels).reshape(train_labels.shape[0], -1)\n",
    "y_test = test_labels.reshape(test_labels.shape[0], -1)\n",
    "\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a939b4aa-d909-47c0-ae8c-e86ab3c15c5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch:  0\n",
      "Epoch:  5\n",
      "Epoch:  10\n",
      "Epoch:  15\n",
      "Epoch:  20\n",
      "Epoch:  25\n",
      "Epoch:  30\n",
      "Epoch:  35\n",
      "Epoch:  40\n",
      "Epoch:  45\n",
      "Epoch:  50\n",
      "Epoch:  55\n",
      "Epoch:  60\n",
      "Epoch:  65\n",
      "Epoch:  70\n",
      "Epoch:  75\n",
      "Epoch:  80\n",
      "Epoch:  85\n",
      "Epoch:  90\n",
      "Epoch:  95\n",
      "Epoch:  100\n",
      "Epoch:  105\n",
      "Epoch:  110\n",
      "Epoch:  115\n",
      "Epoch:  120\n",
      "Epoch:  125\n",
      "Epoch:  130\n",
      "Epoch:  135\n",
      "Epoch:  140\n",
      "Epoch:  145\n",
      "Epoch:  150\n",
      "Epoch:  155\n",
      "Epoch:  160\n",
      "Epoch:  165\n",
      "Epoch:  170\n",
      "Epoch:  175\n",
      "Epoch:  180\n",
      "Epoch:  185\n",
      "Epoch:  190\n",
      "Epoch:  195\n",
      "Epoch:  200\n",
      "Epoch:  205\n",
      "Epoch:  210\n",
      "Epoch:  215\n",
      "Epoch:  220\n",
      "Epoch:  225\n",
      "Epoch:  230\n",
      "Epoch:  235\n",
      "Epoch:  240\n",
      "Epoch:  245\n",
      "Epoch:  250\n",
      "Epoch:  255\n",
      "Epoch:  260\n",
      "Epoch:  265\n",
      "Epoch:  270\n",
      "Epoch:  275\n",
      "Epoch:  280\n",
      "Epoch:  285\n",
      "Epoch:  290\n",
      "Epoch:  295\n",
      "Epoch:  300\n",
      "Epoch:  305\n",
      "Epoch:  310\n",
      "Epoch:  315\n",
      "Epoch:  320\n",
      "Epoch:  325\n",
      "Epoch:  330\n",
      "Epoch:  335\n",
      "Epoch:  340\n",
      "Epoch:  345\n",
      "Epoch:  350\n",
      "Epoch:  355\n",
      "Epoch:  360\n",
      "Epoch:  365\n",
      "Epoch:  370\n",
      "Epoch:  375\n",
      "Epoch:  380\n",
      "Epoch:  385\n",
      "Epoch:  390\n",
      "Epoch:  395\n",
      "Epoch:  400\n",
      "Epoch:  405\n",
      "Epoch:  410\n",
      "Epoch:  415\n",
      "Epoch:  420\n",
      "Epoch:  425\n",
      "Epoch:  430\n",
      "Epoch:  435\n",
      "Epoch:  440\n",
      "Epoch:  445\n",
      "Epoch:  450\n",
      "Epoch:  455\n",
      "Epoch:  460\n",
      "Epoch:  465\n",
      "Epoch:  470\n",
      "Epoch:  475\n",
      "Epoch:  480\n",
      "Epoch:  485\n",
      "Epoch:  490\n",
      "Epoch:  495\n",
      "Epoch:  500\n",
      "Epoch:  505\n",
      "Epoch:  510\n",
      "Epoch:  515\n",
      "Epoch:  520\n",
      "Epoch:  525\n",
      "Epoch:  530\n",
      "Epoch:  535\n",
      "Epoch:  540\n",
      "Epoch:  545\n",
      "Epoch:  550\n",
      "Epoch:  555\n",
      "Epoch:  560\n",
      "Epoch:  565\n",
      "Epoch:  570\n",
      "Epoch:  575\n",
      "Epoch:  580\n",
      "Epoch:  585\n",
      "Epoch:  590\n",
      "Epoch:  595\n",
      "Epoch:  600\n",
      "Epoch:  605\n",
      "Epoch:  610\n",
      "Epoch:  615\n",
      "Epoch:  620\n",
      "Epoch:  625\n",
      "Epoch:  630\n",
      "Epoch:  635\n",
      "Epoch:  640\n",
      "Epoch:  645\n",
      "Epoch:  650\n",
      "Epoch:  655\n",
      "Epoch:  660\n",
      "Epoch:  665\n",
      "Epoch:  670\n",
      "Epoch:  675\n",
      "Epoch:  680\n",
      "Epoch:  685\n",
      "Epoch:  690\n",
      "Epoch:  695\n",
      "(10, 785)\n"
     ]
    }
   ],
   "source": [
    "# Funções de ativação para o neurônio\n",
    "def activate_functions(type, matrix):\n",
    "    if type == 'sigmoid':\n",
    "        return 1 / (1 + np.exp(-matrix))\n",
    "    elif type == 'softmax':\n",
    "        exp_matrix = np.exp(matrix - np.max(matrix, axis=1, keepdims=True))\n",
    "        return exp_matrix / np.sum(exp_matrix, axis=1, keepdims=True)   \n",
    "    elif type == 'tanh':\n",
    "        return np.tanh(matrix)\n",
    "\n",
    "# Função de treino para o classificador perceptron logístico\n",
    "def train_logistic_perceptron(X, y, epochs, l_rate):\n",
    "    weights = np.random.randn(y.shape[1], X.shape[1]) * 0.1 # Matriz com dimensões: num_classes X num_atributos\n",
    "    \n",
    "    for epoch in range(epochs): # Iterando épocas\n",
    "        if epoch % 5 == 0:\n",
    "            print('Epoch: ', epoch)\n",
    "        \n",
    "        z = X_train @ weights.T\n",
    "        result = activate_functions('softmax', z)\n",
    "        error =  result - y # Erro por classe\n",
    "        grad = error / len(X_train)\n",
    "\n",
    "        # Ajustar os pesos para cada classe separadamente\n",
    "        weights -= l_rate * np.dot(grad.T, X_train)\n",
    "\n",
    "    return weights\n",
    "\n",
    "# Treinar o classificador\n",
    "weights = train_logistic_perceptron(X_train, y_train, epochs=700, l_rate=0.01)\n",
    "print(weights.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e977f442-573b-42cb-abfa-4182697b9a5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000,)\n",
      "(10000, 1)\n",
      "(10000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Função de predição usando o classificador linear\n",
    "def predict_logistic_perceptron(X, W):\n",
    "    z = X @ weights.T\n",
    "    result = activate_functions('softmax', z)\n",
    "\n",
    "    # Converte as saídas para as classes preditas (0 a 9) usando a função argmax\n",
    "    # A classe predita será o índice do valor máximo em cada linha\n",
    "    classe = np.argmax(result, axis=1)\n",
    "    print(classe.shape)\n",
    "\n",
    "    return np.expand_dims(classe, axis=1)\n",
    "\n",
    "# Realizar a predição no conjunto de teste\n",
    "y_pred_test = predict_logistic_perceptron(X_test, weights)\n",
    "\n",
    "print(y_pred_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9ae86139-7fcb-4ed0-b1d6-6da142025b13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurácia: 0.9048\n",
      "Taxa de erro: 0.09519999999999995\n"
     ]
    }
   ],
   "source": [
    "# Calcular a acurácia\n",
    "accuracy = np.mean(y_pred_test == y_test)\n",
    "# Imprimir a acurácia\n",
    "print(\"Acurácia:\", accuracy)\n",
    "\n",
    "# Avaliar o desempenho do classificador\n",
    "erro = 1 - (np.sum(y_pred_test == y_test) / len(y_test))\n",
    "print(\"Taxa de erro: {}\".format(erro))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
